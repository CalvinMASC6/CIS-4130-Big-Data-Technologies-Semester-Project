{"cells":[{"cell_type":"markdown","id":"f6097c0c","metadata":{"collapsed":true,"tags":[]},"source":["spark"]},{"cell_type":"code","execution_count":1,"id":"112f454f","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark import SparkContext\n","from google.cloud import storage\n","\n","from pyspark.sql.functions import *\n","from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","import numpy as np\n","import seaborn as sns\n","import io\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"id":"bace94b8","metadata":{},"outputs":[],"source":["# Create SparkContext\n","sc = SparkContext.getOrCreate()\n","\n","# Set the log level to ERROR to suppress INFO messages\n","sc.setLogLevel(\"ERROR\")"]},{"cell_type":"code","execution_count":3,"id":"f2902384","metadata":{},"outputs":[{"data":{"text/html":["<style>pre {white-space: pre !important}</style>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["#fix the formating of the shows, so they don't overlap.\n","def hscroll(activate=True):\n","  \"\"\"activate/deactivate horizontal scrolling for wide output cells\"\"\"\n","  from IPython.display import display, HTML\n","  style = ('pre-wrap','pre')[activate] # select white-space style\n","  display(HTML(\"<style>pre {white-space: %s !important}</style>\" % style))\n","hscroll()"]},{"cell_type":"code","execution_count":4,"id":"2dc5a8b3","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+----------+------------+\n","|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|tip_amount|combined_fee|\n","+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+----------+------------+\n","|       2| 2023-12-31 12:23:25|  2023-12-31 12:31:17|              1|          1.0|         1|           4|          45|           2|       0.0|        15.8|\n","|       2| 2023-12-31 11:57:56|  2023-12-31 12:06:23|              1|         1.42|         1|         238|         142|           1|      2.94|       20.14|\n","|       2| 2023-12-31 12:08:29|  2023-12-31 12:18:00|              1|         0.73|         1|         230|         170|           2|       0.0|        15.8|\n","|       2| 2023-12-31 12:31:32|  2023-12-31 12:54:25|              1|         5.05|         1|         249|         236|           1|      5.67|       39.67|\n","|       2| 2023-12-31 12:23:25|  2023-12-31 12:28:00|              1|         0.69|         1|         163|         142|           1|       2.1|        15.1|\n","+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+----------+------------+\n","only showing top 5 rows\n","\n","None\n"]}],"source":["# Create SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","file_path = f'gs://my-bigdata-project-cm/cleaned/yellow_tripdata_2023-12.parquet'\n","try:\n","        # Read Parquet file from Google Cloud Storage\n","        sdf = spark.read.parquet(file_path)\n","        sdf = sdf.drop('store_and_fwd_flag')\n","        \n","        # Sum up the numerical values of the trip fee columns\n","        sdf = sdf.withColumn(\"combined_fee\", expr(\"total_amount + congestion_surcharge + airport_fee\"))\n","            \n","        # Drop the columns related to the trip fee, as they are now redundant\n","        columns_to_drop = [\"fare_amount\",\"extra\",\"mta_tax\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\"congestion_surcharge\",\"airport_fee\"]\n","        sdf = sdf.drop(*columns_to_drop)\n","        \n","        # Show the first row of the DataFrame\n","        print(sdf.show(5))\n","        \n","except Exception as e:\n","    print(f\"An error occurred on {file_path}:\", str(e))\n","    #spark.stop()"]},{"cell_type":"markdown","id":"e5594bde","metadata":{},"source":["sdf.summary().show()"]},{"cell_type":"markdown","id":"b8ac1078","metadata":{"tags":[]},"source":["# Print schema to see data types of all columns\n","print(sdf.printSchema())"]},{"cell_type":"markdown","id":"ed2890f2","metadata":{},"source":["# Predict the passenger count"]},{"cell_type":"markdown","id":"56943003","metadata":{},"source":["## Indexing(more like new column creation)"]},{"cell_type":"code","execution_count":5,"id":"2266fa58","metadata":{},"outputs":[],"source":["# Creates a hour column as a double\n","def time_index_creator(sdf, input_column):\n","    name = input_column.split(\"_\")[1]\n","    \n","    # Extract hour from datetime column\n","    sdf = sdf.withColumn(f\"{name}_hour_index\", hour(input_column).cast(\"double\"))\n","    \n","    # Extract day of the week from datetime column\n","    sdf = sdf.withColumn(f\"{name}_day_index\", day(input_column).cast(\"double\"))\n","    \n","    # Extract month from datetime column\n","    sdf = sdf.withColumn(f\"{name}_month_index\", month(input_column).cast(\"double\"))\n","    \n","    return sdf\n","\n","# Returns a DataFrame with a new column \"pickup_hour_index\" containing the indexed column of hours for the \"tpep_pickup_datetime\" column\n","sdf = time_index_creator(sdf, \"tpep_pickup_datetime\")\n","# Returns a DataFrame with a new column \"dropoff_hour_index\" containing the indexed column of hours for the \"tpep_dropoff_datetime\" column\n","sdf = time_index_creator(sdf, \"tpep_dropoff_datetime\")\n","\n","# Calculate the duration of the trip in minutes and round it to the 2nd decimal place\n","#sdf = sdf.withColumn(\"trip_length_minutes\", round((unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60, 2))"]},{"cell_type":"markdown","id":"3987aae2","metadata":{},"source":["## Handle outliers"]},{"cell_type":"code","execution_count":6,"id":"9872adf5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#removes rows were outliers are present in the selected column\n","def remove_outliers(df, column_name, threshold=2):\n","    # Calculate mean and standard deviation\n","    mean_val = df.select(mean(col(column_name))).collect()[0][0]\n","    std_val = df.select(stddev(col(column_name))).collect()[0][0]\n","\n","    # Calculate lower and upper bounds\n","    lower_bound = mean_val - threshold * std_val\n","    upper_bound = mean_val + threshold * std_val\n","\n","    # Filter out rows outside the threshold\n","    filtered_df = df.filter((col(column_name) >= lower_bound) & (col(column_name) <= upper_bound))\n","\n","    return filtered_df\n","\n","#removes outliers from the passenger_count column\n","sdf = remove_outliers(sdf, 'passenger_count')\n","#removes outliers from the trip_distance column\n","sdf = remove_outliers(sdf, 'trip_distance')\n","#removes outliers from the RatecodeID column\n","sdf = remove_outliers(sdf, 'RatecodeID')\n","#removes outliers from the tip_amount column\n","sdf = remove_outliers(sdf, 'tip_amount')\n","#removes outliers from the combined_fee column\n","sdf = remove_outliers(sdf, 'combined_fee')\n","#removes outliers from the trip_length_minutes column\n","#sdf = remove_outliers(sdf, 'trip_length_minutes')\n","\n","\n","# Filter out rows where the absolute difference between dropoff_month_index and pickup_month_index is more than 2\n","sdf = sdf.filter(abs(col(\"dropoff_month_index\") - col(\"pickup_month_index\")) <= 2)"]},{"cell_type":"code","execution_count":7,"id":"c600ac84","metadata":{},"outputs":[],"source":["# Split the data into training and test sets\n","trainingData, testData = sdf.randomSplit([0.70, 0.3], seed=42)"]},{"cell_type":"markdown","id":"ca319a22","metadata":{},"source":["## Encoding"]},{"cell_type":"code","execution_count":8,"id":"e362ef8e","metadata":{},"outputs":[],"source":["# Create an encoder for the three indexes and the age integer column.\n","encoder = OneHotEncoder(inputCols=[\"pickup_hour_index\", \"dropoff_hour_index\",'pickup_day_index','dropoff_day_index','pickup_month_index','dropoff_month_index', \"VendorID\", \"RatecodeID\", \"PULocationID\", \"DOLocationID\", \"payment_type\", \"passenger_count\"],\n","                        outputCols=[\"pickupHourVector\", \"dropoffHourVector\",'pickupDayVector','dropoffDayVector','pickupMonthVector','dropoffMonthVector', \"VendorIDVector\", \"RatecodeIDVector\", \"PULocationIDVector\", \"DOLocationIDVector\", \"paymentTypeVector\", \"passengerCountVector\"], dropLast=True, handleInvalid=\"keep\")"]},{"cell_type":"markdown","id":"baaca7d3","metadata":{},"source":["## Scaling"]},{"cell_type":"code","execution_count":9,"id":"d7040528","metadata":{},"outputs":[],"source":["#scale tripLengthVector\n","#tripLength_scaler = MinMaxScaler(inputCol=\"tripLengthVector\", outputCol=\"tripLengthScaled\")\n","\n","#scale passengerCountVector\n","passengerCount_scaler = MinMaxScaler(inputCol=\"passengerCountVector\", outputCol=\"passengerCountScaled\")\n","\n","#scale pickupHourVector\n","pickupHour_scaler = MinMaxScaler(inputCol=\"pickupHourVector\", outputCol=\"pickupHourScaled\")\n","\n","#scale dropoffHourVector\n","dropoffHour_scaler = MinMaxScaler(inputCol=\"dropoffHourVector\", outputCol=\"dropoffHourScaled\")\n","\n","#scale pickupDayVector\n","pickupDay_scaler = MinMaxScaler(inputCol=\"pickupDayVector\", outputCol=\"pickupDayScaled\")\n","\n","#scale dropoffDayVector\n","dropoffDay_scaler = MinMaxScaler(inputCol=\"dropoffDayVector\", outputCol=\"dropoffDayScaled\")\n","\n","#scale pickupMonthVector\n","pickupMonth_scaler = MinMaxScaler(inputCol=\"pickupMonthVector\", outputCol=\"pickupMonthScaled\")\n","\n","#scale dropoffMonthVector\n","dropoffMonth_scaler = MinMaxScaler(inputCol=\"dropoffMonthVector\", outputCol=\"dropoffMonthScaled\")\n","\n","# scale tip_amount(which is my label column)\n","#tip_amount_scaler = MinMaxScaler(inputCol=\"tip_amount\", outputCol=\"scaled_tip_amount\")"]},{"cell_type":"code","execution_count":10,"id":"66a2723f","metadata":{},"outputs":[],"source":["#assemble and scale trip_distance\n","distance_assembler = VectorAssembler(inputCols=['trip_distance'], outputCol='tripDistanceVector')\n","distance_scaler = MinMaxScaler(inputCol=\"tripDistanceVector\", outputCol=\"tripDistanceScaled\")\n","\n","#assemble and scale combined_fee\n","fee_assembler= VectorAssembler(inputCols=['combined_fee'], outputCol='combinedFeeVector')\n","fee_scaler = MinMaxScaler(inputCol=\"combinedFeeVector\", outputCol=\"combinedFeeScaled\")\n","\n","#assemble and scale tip_amount(which is my label column)\n","#tip_amount_assembler = VectorAssembler(inputCols=['tip_amount'], outputCol='tipAmountVector')\n","#tip_amount_scaler = MinMaxScaler(inputCol=\"tipAmountVector\", outputCol=\"tipAmountScaled\")"]},{"cell_type":"markdown","id":"bd47b823","metadata":{},"source":["## Aassembling "]},{"cell_type":"code","execution_count":11,"id":"522e15ef","metadata":{},"outputs":[],"source":["# Create an assembler for the individual feature vectors and the float/double columns\n","assembler = VectorAssembler(inputCols=['VendorIDVector','RatecodeIDVector','PULocationIDVector','DOLocationIDVector','paymentTypeVector','passengerCountScaled','pickupHourScaled','dropoffHourScaled','pickupDayScaled', 'dropoffDayScaled', 'pickupMonthScaled', 'dropoffMonthScaled', 'tripDistanceScaled','combinedFeeScaled'], outputCol=\"features\")"]},{"cell_type":"markdown","id":"99477892","metadata":{},"source":["## Model building & testing"]},{"cell_type":"code","execution_count":12,"id":"9ce5f6a8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of models to be tested:  50\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Average metric [1.2435997173421156]\n"]},{"name":"stderr","output_type":"stream","text":["24/05/17 18:07:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","[Stage 227:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+--------------------+\n","|tip_amount|prediction          |\n","+----------+--------------------+\n","|5.8       |4.433393962707913   |\n","|6.1       |4.934487490275649   |\n","|4.8       |4.135689210301461   |\n","|8.65      |5.993866963498278   |\n","|0.0       |1.3887921734319102  |\n","|2.85      |2.6868574851654907  |\n","|0.0       |-0.651030005812232  |\n","|5.3       |3.8818182934616443  |\n","|0.0       |-0.01817632609427422|\n","|2.3       |2.192734575355071   |\n","|5.65      |4.624445308334251   |\n","|0.0       |1.1873125772877495  |\n","|2.7       |2.509090901293873   |\n","|2.25      |2.648925516742933   |\n","|2.85      |2.5930628545577017  |\n","|7.0       |4.302700908996459   |\n","|0.0       |3.0378740825706645  |\n","|3.0       |2.7525484829816804  |\n","|4.1       |3.387953001459974   |\n","|3.0       |2.7803596166855202  |\n","+----------+--------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Create a Ridge Regression Estimator\n","ridge_reg = LinearRegression(labelCol='tip_amount',  elasticNetParam=0, regParam=0.1)\n","\n","# Create a regression evaluator (to get RMSE, R2, RME, etc.)\n","evaluator = RegressionEvaluator(labelCol='tip_amount')\n","\n","# Create the pipeline Indexer is stage 0 and Ridge Regression (ridge_reg)  is stage 3\n","regression_pipe = Pipeline(stages=[encoder, passengerCount_scaler, pickupHour_scaler, dropoffHour_scaler, pickupDay_scaler, dropoffDay_scaler, pickupMonth_scaler, dropoffMonth_scaler, distance_assembler, distance_scaler, fee_assembler, fee_scaler, assembler, ridge_reg])\n","\n","# Create a grid to hold hyperparameters \n","grid = ParamGridBuilder()\n","\n","# Two ways to try .fitIntercept\n","params = ParamGridBuilder() \\\n",".addGrid(ridge_reg.fitIntercept, [True, False]) \\\n",".addGrid(ridge_reg.regParam, [0.001, 0.01, 0.1, 1, 10]) \\\n",".addGrid(ridge_reg.elasticNetParam, [0, 0.25, 0.5, 0.75, 1]) \\\n",".build()\n","\n","# Build the parameter grid\n","grid = grid.build()\n","\n","print('Number of models to be tested: ', len(params))\n","\n","# Create the CrossValidator using the hyperparameter grid\n","cv = CrossValidator(estimator=regression_pipe, \n","                    estimatorParamMaps=grid, \n","                    evaluator=evaluator, \n","                    numFolds=5,seed=42)\n","\n","# Train the models\n","all_models  = cv.fit(trainingData)\n","\n","# Show the average performance over the five folds for each grid combination\n","print(f\"Average metric {all_models.avgMetrics}\")\n","\n","# Get the best model from all of the models trained\n","bestModel = all_models.bestModel\n","\n","# Use the model 'bestModel' to predict the test set\n","test_results = bestModel.transform(testData)\n","\n","# Show the predicted tip\n","test_results.select('tip_amount', 'prediction').show(truncate=False)"]},{"cell_type":"code","execution_count":13,"id":"678f5034","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 229:>                                                        (0 + 2) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["RMSE: 1.2413312413007667  R-squared:0.6905267891954869\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 229:============================>                            (1 + 1) / 2]\r","\r","                                                                                \r"]}],"source":["# RMSE measures the differences between what the model predicted ('prediction') and the actual values ('tip').\n","rmse = evaluator.evaluate(test_results, {evaluator.metricName:'rmse'})\n","# R-Squared measures how much of the variability in the target variable (tip) can be explained by the model\n","r2 =evaluator.evaluate(test_results,{evaluator.metricName:'r2'})\n","print(f\"RMSE: {rmse}  R-squared:{r2}\")"]},{"cell_type":"markdown","id":"15c707e0","metadata":{},"source":["# Save the best model\n","model_path = \"gs://my-bigdata-project-cm/models/taxi_tip_linear_regression_model_v2\"\n","bestModel.write().overwrite().save(model_path)"]},{"cell_type":"markdown","id":"5ce673e3","metadata":{},"source":["print(bestModel.stages)\n","\n","coefficients = bestModel.stages[3].coefficients\n","print(\"bestModel coefficients\", coefficients)\n","intercept = bestModel.stages[3].intercept\n","print(\"bestModel intercept\", intercept)"]},{"cell_type":"markdown","id":"01a105dc","metadata":{"collapsed":true},"source":["# Visualize regression results\n","\n","# The Spark dataframe test_results holds the original 'tip' as well as the 'prediction'\n","# Select and convert to a Pandas dataframe\n","df = test_results.select('tip_amount','prediction').toPandas()\n","\n","# Set the style for Seaborn plots\n","sns.set_style(\"white\")\n"," \n","# Create a relationship plot between tip and prediction\n","sns.lmplot(x='tip_amount', y='prediction', data=df)"]},{"cell_type":"markdown","id":"efc074c5","metadata":{"collapsed":true},"source":["# residuals = bestModel.stages[3].residuals\n","# test_results.residuals\n","\n","df = test_results.select('tip_amount','prediction').toPandas()\n","df['residuals'] = df['tip_amount'] - df['prediction']\n","\n","# Set the style for Seaborn plots\n","sns.set_style(\"white\")\n"," \n","# Create a relationship plot between tip and prediction\n","sns.regplot(x = 'prediction', y = 'residuals', data = df, scatter = True, color = 'red') "]},{"cell_type":"markdown","id":"988ce0dc","metadata":{"collapsed":true},"source":["# TODO: Add more visualizations for Regression performance metrics\n","# Loop through the features to extract the original column names. Store in the var_index dictionary\n","var_index = dict()\n","for variable_type in ['numeric', 'binary']:\n","    for variable in test_results.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][variable_type]:\n","         print(\"Found variable:\", variable)\n","         idx = variable['idx']\n","         name = variable['name']\n","         var_index[idx] = name      # Add the name of the column to the dictionary\n","\n","# Loop through all of the variables found and print out the associated coefficients\n","for i in range(len(var_index)):\n","    print(i, var_index[i], coeff[i])"]},{"cell_type":"markdown","id":"f3a178d2","metadata":{},"source":["# Close connection to Spark\n","spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}