{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a01724",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112f454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from google.cloud import storage\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bace94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Set the log level to ERROR to suppress INFO messages\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2902384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre {white-space: pre !important}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fix the formating of the shows, so they don't overlap.\n",
    "def hscroll(activate=True):\n",
    "  \"\"\"activate/deactivate horizontal scrolling for wide output cells\"\"\"\n",
    "  from IPython.display import display, HTML\n",
    "  style = ('pre-wrap','pre')[activate] # select white-space style\n",
    "  display(HTML(\"<style>pre {white-space: %s !important}</style>\" % style))\n",
    "hscroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc5a8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2011-01-01 15:44:50|  2011-01-01 16:13:58|              2|         17.2|         3|         170|           1|           1|       53.5|  0.0|    0.0|      9.52|        10.0|                  0.0|       73.02|                 0.0|        0.0|\n",
      "|       2| 2011-01-05 05:42:00|  2011-01-05 06:07:00|              2|        17.54|         3|         186|           1|           1|       53.5|  0.5|    0.0|      13.5|         8.0|                  0.0|        75.5|                 0.0|        0.0|\n",
      "|       2| 2011-01-05 15:46:00|  2011-01-05 16:21:00|              1|        17.52|         3|         170|           1|           2|       55.5|  0.0|    0.0|       0.0|         4.8|                  0.0|        60.3|                 0.0|        0.0|\n",
      "|       1| 2011-01-07 15:00:47|  2011-01-07 15:26:37|              1|         14.5|         5|         186|           1|           1|       75.0|  0.0|    0.0|     11.25|         0.0|                  0.0|       86.25|                 0.0|        0.0|\n",
      "|       2| 2011-01-11 11:12:00|  2011-01-11 11:41:00|              1|        16.05|         3|         234|           1|           2|       51.1|  0.0|    0.0|       0.0|        12.0|                  0.0|        63.1|                 0.0|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "file_path = f'gs://my-bigdata-project-cm/cleaned/yellow_tripdata_2011-01.parquet'\n",
    "try:\n",
    "        # Read Parquet file from Google Cloud Storage\n",
    "        sdf = spark.read.parquet(file_path)\n",
    "        sdf = sdf.drop('store_and_fwd_flag')\n",
    "        \n",
    "        # Sum up the numerical values of the trip fee columns\n",
    "        sdf = sdf.withColumn(\"combined_fee\",\n",
    "            sum(sdf[col] for col in [\"total_amount\", \"congestion_surcharge\", \"airport_fee\"]).cast(\"double\"))\n",
    "        # Drop the columns related to the trip fee, as they are now redundant\n",
    "        columns_to_drop = [\"fare_amount\",\"extra\",\"mta_tax\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\"congestion_surcharge\",\"airport_fee\"]\n",
    "        sdf = sdf.drop(*columns_to_drop)\n",
    "        \n",
    "        # Show the first row of the DataFrame\n",
    "        print(sdf.show(5))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred on {file_path}:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316d993c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sdf\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sdf' is not defined"
     ]
    }
   ],
   "source": [
    "sdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491a673",
   "metadata": {},
   "source": [
    "## Handle outliers - TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfaa3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b500596",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Print schema to see data types of all columns\n",
    "print(sdf.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2890f2",
   "metadata": {},
   "source": [
    "# Predict the pasanger count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56943003",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88c36a",
   "metadata": {},
   "source": [
    "# Creates a hour column as a double\n",
    "def hour_index(sdf, input_column):\n",
    "    name = input_column.split(\"_\")[1]\n",
    "    \n",
    "    # Extract hour from datetime column\n",
    "    sdf = sdf.withColumn(f\"{name}_hour_index\", hour(input_column).cast(\"double\"))\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "# Returns a DataFrame with a new column \"pickup_hour_index\" containing the indexed column of hours for the \"tpep_pickup_datetime\" column\n",
    "sdf = hour_index(sdf, \"tpep_pickup_datetime\")\n",
    "# Returns a DataFrame with a new column \"dropoff_hour_index\" containing the indexed column of hours for the \"tpep_dropoff_datetime\" column\n",
    "sdf = hour_index(sdf, \"tpep_dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a hour column as a double\n",
    "def time_index_creator(sdf, input_column):\n",
    "    name = input_column.split(\"_\")[1]\n",
    "    \n",
    "    # Extract hour from datetime column\n",
    "    sdf = sdf.withColumn(f\"{name}_hour_index\", hour(input_column).cast(\"double\"))\n",
    "    \n",
    "    # Extract day of the week from datetime column\n",
    "    sdf = sdf.withColumn(f\"{name}_day_index\", day(input_column).cast(\"double\"))\n",
    "    \n",
    "    # Extract month from datetime column\n",
    "    sdf = sdf.withColumn(f\"{name}_month_index\", month(input_column).cast(\"double\"))\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "# Returns a DataFrame with a new column \"pickup_hour_index\" containing the indexed column of hours for the \"tpep_pickup_datetime\" column\n",
    "sdf = time_index_creator(sdf, \"tpep_pickup_datetime\")\n",
    "# Returns a DataFrame with a new column \"dropoff_hour_index\" containing the indexed column of hours for the \"tpep_dropoff_datetime\" column\n",
    "sdf = time_index_creator(sdf, \"tpep_dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e30a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "trainingData, testData = sdf.randomSplit([0.70, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5681a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca319a22",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f22678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder for the three indexes and the age integer column.\n",
    "encoder = OneHotEncoder(inputCols=[\"pickup_hour_index\", \"dropoff_hour_index\",'pickup_day_index','dropoff_day_index','pickup_month_index','dropoff_month_index', \"VendorID\", \"RatecodeID\", \"PULocationID\", \"DOLocationID\", \"payment_type\", \"passenger_count\"],\n",
    "                        outputCols=[\"pickupHourVector\", \"dropoffHourVector\",'pickupDayVector','dropoffDayVector','pickupMonthVector','dropoffMonthVector', \"VendorIDVector\", \"RatecodeIDVector\", \"PULocationIDVector\", \"DOLocationIDVector\", \"paymentTypeVector\", \"passengerCountVector\"], dropLast=True, handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaca7d3",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad4fe9",
   "metadata": {},
   "source": [
    "altered_names = []\n",
    "for i in columns_to_minmax:\n",
    "    words = i.split(\"_\")#remove the underscores\n",
    "    # Capitalize the first letter of the second word\n",
    "    if len(words) > 1:\n",
    "        words[1] = words[1].capitalize()\n",
    "    altered_names.append(\"\".join(words))\n",
    "\n",
    "assemblers = {}\n",
    "scalers = {}\n",
    "#vector assembele than scale the columns_to_minmax.\n",
    "for col_index in range(len(columns_to_minmax)):\n",
    "    assemblers[altered_names[col_index]] = VectorAssembler(inputCols=[columns_to_minmax[col_index]], outputCol=f\"{altered_names[col_index]}Vector\")\n",
    "    scalers[altered_names[col_index]] = MinMaxScaler(inputCol={assemblers[col_index]}, outputCol=f\"{altered_names[col_index]}Scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b816fad",
   "metadata": {},
   "source": [
    "columns_to_minmax = ['trip_distance','fare_amount','extra','mta_tax','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge','airport_fee']\n",
    "\n",
    "# Scale the columns_to_minmax\n",
    "assembler_columns = VectorAssembler(inputCols=columns_to_minmax, outputCol='columnsVector')\n",
    "columns_scaler = MinMaxScaler(inputCol=\"columnsVector\", outputCol=\"columnsScaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314774a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble and scale trip_distance\n",
    "assembler_columns = VectorAssembler(inputCols=['trip_distance'], outputCol='tripDistanceVector')\n",
    "columns_scaler = MinMaxScaler(inputCol=\"tripDistanceVector\", outputCol=\"tripDistanceScaled\")\n",
    "\n",
    "#assemble and scale combined_fee\n",
    "assembler_columns = VectorAssembler(inputCols=['combined_fee'], outputCol='combinedFeeVector')\n",
    "columns_scaler = MinMaxScaler(inputCol=\"combinedFeeVector\", outputCol=\"combinedFeeScaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47b823",
   "metadata": {},
   "source": [
    "## Aassembling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b19b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an assembler for the individual feature vectors and the float/double columns\n",
    "assembler = VectorAssembler(inputCols=['pickupHourVector','dropoffHourVector','VendorIDVector','RatecodeIDVector', 'PULocationIDVector', 'DOLocationIDVector', 'paymentTypeVector', 'passengerCountVector', 'tripDistanceScaled','combinedFeeScaled'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f78ba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average metric [0.9067747474975025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 43:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------+\n",
      "|total_amount|tip_amount|prediction          |\n",
      "+------------+----------+--------------------+\n",
      "|5.9         |0.0       |-0.25321305015598705|\n",
      "|8.3         |0.0       |-0.13185146219096977|\n",
      "|10.46       |1.36      |1.711747975113023   |\n",
      "|15.1        |0.0       |0.2728012422181101  |\n",
      "|12.7        |0.0       |0.1208213161713998  |\n",
      "|17.9        |0.0       |0.49510423627183653 |\n",
      "|21.5        |0.0       |0.6476153939390767  |\n",
      "|21.37       |4.27      |2.5594969527895763  |\n",
      "|18.7        |0.0       |0.5029547479272758  |\n",
      "|9.1         |0.0       |-0.29006132089966086|\n",
      "|11.1        |0.0       |0.05617593030326207 |\n",
      "|3.9         |0.0       |-0.41887240601661535|\n",
      "|15.1        |0.0       |0.3793553028241545  |\n",
      "|18.7        |0.0       |0.4953396631852629  |\n",
      "|3.5         |0.0       |-0.8257632315042596 |\n",
      "|13.5        |0.0       |0.20562018370896062 |\n",
      "|4.3         |0.0       |-0.4420384605630854 |\n",
      "|12.7        |0.0       |0.08599473560534565 |\n",
      "|9.1         |0.0       |-0.08013433780482992|\n",
      "|11.9        |1.2       |1.79455970029159    |\n",
      "+------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Ridge Regression Estimator\n",
    "ridge_reg = LinearRegression(labelCol='tip_amount',  elasticNetParam=0, regParam=0.1)\n",
    "\n",
    "# Create a regression evaluator (to get RMSE, R2, RME, etc.)\n",
    "evaluator = RegressionEvaluator(labelCol='tip_amount')\n",
    "\n",
    "# Create the pipeline Indexer is stage 0 and Ridge Regression (ridge_reg)  is stage 3\n",
    "regression_pipe = Pipeline(stages=[encoder, assembler_columns, columns_scaler, assembler, ridge_reg])\n",
    "\n",
    "# Create a grid to hold hyperparameters \n",
    "grid = ParamGridBuilder()\n",
    "\n",
    "# Two ways to try .fitIntercept\n",
    "params = ParamGridBuilder() \\\n",
    ".addGrid(ridge_reg.fitIntercept, [True, False]) \\\n",
    ".addGrid(ridge_reg.regParam, [0.001, 0.01, 0.1, 1, 10]) \\\n",
    ".addGrid(ridge_reg.elasticNetParam, [0, 0.25, 0.5, 0.75, 1]) \\\n",
    ".build()\n",
    "\n",
    "# Build the parameter grid\n",
    "grid = grid.build()\n",
    "\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "# Create the CrossValidator using the hyperparameter grid\n",
    "cv = CrossValidator(estimator=regression_pipe, \n",
    "                    estimatorParamMaps=grid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5,seed=42)\n",
    "\n",
    "# Train the models\n",
    "all_models  = cv.fit(trainingData)\n",
    "\n",
    "# Show the average performance over the three folds for each grid combination\n",
    "print(f\"Average metric {all_models.avgMetrics}\")\n",
    "\n",
    "# Get the best model from all of the models trained\n",
    "bestModel = all_models.bestModel\n",
    "\n",
    "# Use the model 'bestModel' to predict the test set\n",
    "test_results = bestModel.transform(testData)\n",
    "\n",
    "# Show the predicted tip\n",
    "test_results.select('tip_amount', 'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560517f",
   "metadata": {},
   "source": [
    "# Save the best model\n",
    "model_path = \"gs://my-bigdata-project-cm/models/taxi_tip_linear_regression_model_v2\"\n",
    "bestModel.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9527821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9258666059008394  R-squared:0.6711496845510172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE measures the differences between what the model predicted ('prediction') and the actual values ('tip').\n",
    "rmse = evaluator.evaluate(test_results, {evaluator.metricName:'rmse'})\n",
    "# R-Squared measures how much of the variability in the target variable (tip) can be explained by the model\n",
    "r2 =evaluator.evaluate(test_results,{evaluator.metricName:'r2'})\n",
    "print(f\"RMSE: {rmse}  R-squared:{r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f7ece",
   "metadata": {},
   "source": [
    "print(bestModel.stages)\n",
    "\n",
    "coefficients = bestModel.stages[3].coefficients\n",
    "print(\"bestModel coefficients\", coefficients)\n",
    "intercept = bestModel.stages[3].intercept\n",
    "print(\"bestModel intercept\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2caf0b",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualize regression results\n",
    "\n",
    "# The Spark dataframe test_results holds the original 'tip' as well as the 'prediction'\n",
    "# Select and convert to a Pandas dataframe\n",
    "df = test_results.select('tip_amount','prediction').toPandas()\n",
    "\n",
    "# Set the style for Seaborn plots\n",
    "sns.set_style(\"white\")\n",
    " \n",
    "# Create a relationship plot between tip and prediction\n",
    "sns.lmplot(x='tip_amount', y='prediction', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb6d12",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# residuals = bestModel.stages[3].residuals\n",
    "# test_results.residuals\n",
    "\n",
    "df = test_results.select('tip_amount','prediction').toPandas()\n",
    "df['residuals'] = df['tip_amount'] - df['prediction']\n",
    "\n",
    "# Set the style for Seaborn plots\n",
    "sns.set_style(\"white\")\n",
    " \n",
    "# Create a relationship plot between tip and prediction\n",
    "sns.regplot(x = 'prediction', y = 'residuals', data = df, scatter = True, color = 'red') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ce0dc",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO: Add more visualizations for Regression performance metrics\n",
    "# Loop through the features to extract the original column names. Store in the var_index dictionary\n",
    "var_index = dict()\n",
    "for variable_type in ['numeric', 'binary']:\n",
    "    for variable in test_results.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][variable_type]:\n",
    "         print(\"Found variable:\", variable)\n",
    "         idx = variable['idx']\n",
    "         name = variable['name']\n",
    "         var_index[idx] = name      # Add the name of the column to the dictionary\n",
    "\n",
    "# Loop through all of the variables found and print out the associated coefficients\n",
    "for i in range(len(var_index)):\n",
    "    print(i, var_index[i], coeff[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a178d2",
   "metadata": {},
   "source": [
    "# Close connection to Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
