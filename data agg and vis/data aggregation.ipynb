{"cells":[{"cell_type":"code","execution_count":1,"id":"21e5a25b","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark import SparkContext\n","from google.cloud import storage\n","from pyspark.sql.functions import col, mean, stddev, expr, date_format, count, avg\n","from pyspark.sql import DataFrame"]},{"cell_type":"code","execution_count":2,"id":"ff435302","metadata":{},"outputs":[],"source":["# Create SparkContext\n","sc = SparkContext.getOrCreate()\n","\n","# Set the log level to ERROR to suppress INFO messages\n","sc.setLogLevel(\"ERROR\")"]},{"cell_type":"code","execution_count":3,"id":"eb127c56","metadata":{},"outputs":[{"data":{"text/html":["<style>pre {white-space: pre !important}</style>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["#fix the formating of the shows, so they don't overlap.\n","def hscroll(activate=True):\n","  \"\"\"activate/deactivate horizontal scrolling for wide output cells\"\"\"\n","  from IPython.display import display, HTML\n","  style = ('pre-wrap','pre')[activate] # select white-space style\n","  display(HTML(\"<style>pre {white-space: %s !important}</style>\" % style))\n","hscroll()"]},{"cell_type":"code","execution_count":4,"id":"047a50b1","metadata":{},"outputs":[],"source":["#removes rows were outliers are present in the selected column\n","def remove_outliers_in_columns(df, column_name, threshold=2):\n","    # Calculate mean and standard deviation\n","    mean_val = df.select(mean(col(column_name))).collect()[0][0]\n","    std_val = df.select(stddev(col(column_name))).collect()[0][0]\n","\n","    # Calculate lower and upper bounds\n","    lower_bound = mean_val - threshold * std_val\n","    upper_bound = mean_val + threshold * std_val\n","\n","    # Filter out rows outside the threshold\n","    filtered_df = df.filter((col(column_name) >= lower_bound) & (col(column_name) <= upper_bound))\n","\n","    return filtered_df"]},{"cell_type":"code","execution_count":18,"id":"c2a5b0e9","metadata":{"scrolled":true},"outputs":[{"ename":"ValueError","evalue":"('Iterator has already started', <google.api_core.page_iterator.HTTPIterator object at 0x7f5bdfe06990>)","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)","Cell \u001B[0;32mIn[18], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#run through the my-bigdata-project-cm bucket.\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mblob\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mblobs\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgs://my-bigdata-project-cm/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mblob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#exscluding the file title.\u001B[39;49;00m\n","File \u001B[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/google/api_core/page_iterator.py:223\u001B[0m, in \u001B[0;36mIterator.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Iterator for each item returned.\u001B[39;00m\n\u001B[1;32m    215\u001B[0m \n\u001B[1;32m    216\u001B[0m \u001B[38;5;124;03mReturns:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;124;03m    ValueError: If the iterator has already been started.\u001B[39;00m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_started:\n\u001B[0;32m--> 223\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIterator has already started\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_started \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_items_iter()\n","\u001B[0;31mValueError\u001B[0m: ('Iterator has already started', <google.api_core.page_iterator.HTTPIterator object at 0x7f5bdfe06990>)"]}],"source":["# Create SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Initialize DataFrames to hold the joined results\n","tp_pc_borough_time_df = None\n","#trips_over_time_df = None\n","\n","#import the boroughs_sdf dataframe\n","boroughs_sdf = spark.read.csv('gs://my-bigdata-project-cm/external_data/taxi_zone_lookup.csv', inferSchema=True, header=True)\n","columns_to_drop = ['Zone','service_zone']\n","boroughs_sdf = boroughs_sdf.drop(*columns_to_drop)\n","\n","# Create a client object that points to GCS\n","storage_client = storage.Client()\n","\n","# Get a list of the 'blobs' (objects or files) in the bucket\n","blobs = storage_client.list_blobs('my-bigdata-project-cm', prefix=\"cleaned/\")\n","\n","#run through the my-bigdata-project-cm bucket.\n","for blob in blobs:\n","    file_path = f'gs://my-bigdata-project-cm/{blob.name}'\n","    \n","    #exscluding the file title.\n","    if not blob.name.endswith('.parquet') or blob.name == 'cleaned/':\n","        print(f\"Skipping file {blob.name}\")\n","        continue\n","        \n","    try:\n","        # Read Parquet file from Google Cloud Storage\n","        sdf = spark.read.parquet(file_path)\n","        #sdf = sdf.sample(False, 0.2)\n","        \n","        #drop unneeded columns\n","        columns_to_drop = ['store_and_fwd_flag', 'VendorID']\n","        sdf = sdf.drop(*columns_to_drop)\n","        \n","        # Sum up the numerical values of the trip fee columns\n","        sdf = sdf.withColumn(\"total_amount\", expr(\"total_amount + congestion_surcharge + airport_fee\"))\n","            \n","        # Drop the columns related to the trip fee, as they are now redundant\n","        columns_to_drop = [\"fare_amount\",\"extra\",\"mta_tax\",\"tolls_amount\",\"improvement_surcharge\",\"congestion_surcharge\",\"airport_fee\"]\n","        sdf = sdf.drop(*columns_to_drop)\n","        \n","        \n","        print(f\"Processing {blob.name}:\")\n","        \n","    except Exception as e:\n","        print(f\"An error occurred on {blob.name}:\", str(e))\n","        continue\n","\n","    #removes outliers from the passenger_count column\n","    sdf = remove_outliers_in_columns(sdf, 'passenger_count')\n","    #removes outliers from the trip_distance column\n","    sdf = remove_outliers_in_columns(sdf, 'trip_distance')\n","    #removes outliers from the tip_amount column\n","    sdf = remove_outliers_in_columns(sdf, 'tip_amount')\n","\n","    # Calculate the absolute difference in months between pickup and dropoff datetime\n","    sdf = sdf.withColumn(\"pickup_dropoff_month_diff\", expr(\"abs(months_between(tpep_dropoff_datetime, tpep_pickup_datetime))\"))\n","    # Filter out rows where the absolute difference is more than 2 months\n","    sdf = sdf.filter(col(\"pickup_dropoff_month_diff\") <= 2)\n","    sdf = sdf.drop('pickup_dropoff_month_diff')\n","    \n","    # Joining boroughs_sdf with boroughs_sdf based on PULocationID\n","    sdf = sdf.join(boroughs_sdf, sdf['PULocationID'] == boroughs_sdf['LocationID'], 'left') \\\n","        .drop('LocationID') \\\n","        .withColumnRenamed('Borough', 'Pickup_Borough')\n","\n","    # Joining boroughs_sdf with trips_with_pickup_borough based on DOLocationID\n","    #sdf = sdf.join(boroughs_sdf, sdf['DOLocationID'] == boroughs_sdf['LocationID'], 'left') \\\n","    #    .drop('LocationID') \\\n","    #    .withColumnRenamed('Borough', 'Dropoff_Borough')\n","    \n","\n","    # Creating new columns for pickup_month, pickup_year, and pickup_hour using date_format\n","    sdf = sdf.withColumn('pickup_month', date_format('tpep_pickup_datetime', 'MM'))\n","    sdf = sdf.withColumn('pickup_year', date_format('tpep_pickup_datetime', 'yyyy'))\n","    sdf = sdf.withColumn('pickup_hour', date_format('tpep_pickup_datetime', 'HH'))\n","\n","    # Aggregations remain the same as before\n","    tp_pc_borough_time = sdf.groupBy('Pickup_Borough', 'pickup_hour', 'pickup_year', 'pickup_month').agg(avg('passenger_count'), avg('tip_amount'), avg('total_amount'),count('*'))\n","    #trips_over_time = sdf.groupBy('pickup_hour', 'pickup_year', 'pickup_month').agg(count('*'))\n","\n","    # Combine the DataFrames each loop\n","    if tp_pc_borough_time_df is not None:\n","        tp_pc_borough_time_df = tp_pc_borough_time_df.union(tp_pc_borough_time)\n","    else:\n","        tp_pc_borough_time_df = tp_pc_borough_time\n","\n","    #if trips_over_time_df is not None:\n","    #    trips_over_time_df = trips_over_time_df.union(trips_over_time)\n","    #else:\n","    #    trips_over_time_df = trips_over_time\n","    #break\n","\n","# Define the file paths in GCS to save the DataFrames\n","#file_path_tp_pc_borough_time = 'gs://my-bigdata-project-cm/aggregated/tp_pc_borough_time.parquet'\n","#file_path_trips_over_time = 'gs://my-bigdata-project-cm/aggregated/trips_over_time.parquet'\n","\n","# Save the DataFrames to Parquet format\n","#tp_pc_borough_time_df.write.parquet(file_path_tp_pc_borough_time, mode='overwrite')\n","#trips_over_time_df.write.parquet(file_path_trips_over_time, mode='overwrite')\n","\n","# Display the first few rows of the Pandas DataFrame\n","print(f'tp_pc_borough_time_df:{tp_pc_borough_time_df.count()}')\n","print(tp_pc_borough_time_df.show(5))\n","#print(f'trips_over_time_df:{trips_over_time_df.count()}')\n","#print(trips_over_time_df.show(5))"]},{"cell_type":"markdown","id":"09997ce2","metadata":{},"source":["# Iterate over each column in tp_pc_borough_time_df\n","for col_name in tp_pc_borough_time_df.columns:\n","    # Find the distinct values in the column\n","    distinct_values = tp_pc_borough_time_df.select(col_name).distinct().collect()\n","    # Print the column name and its distinct values\n","    print(f\"Unique values in column '{col_name}':\")\n","    for row in distinct_values:\n","        print(row[0])\n"]},{"cell_type":"markdown","id":"8032e356","metadata":{},"source":["# Define the file paths in GCS to save the DataFrames\n","file_path_tp_pc_borough_time = 'gs://my-bigdata-project-cm/aggregated/tp_pc_borough_time.parquet'\n","file_path_trips_over_time = 'gs://my-bigdata-project-cm/aggregated/trips_over_time.parquet'\n","\n","# Save the DataFrames to Parquet format\n","tp_pc_borough_time_df.write.parquet(file_path_tp_pc_borough_time, mode='overwrite')\n","trips_over_time_df.write.parquet(file_path_trips_over_time, mode='overwrite')\n"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}