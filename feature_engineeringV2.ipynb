{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6097c0c",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112f454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from google.cloud import storage\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bace94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Set the log level to ERROR to suppress INFO messages\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2902384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre {white-space: pre !important}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fix the formating of the shows, so they don't overlap.\n",
    "def hscroll(activate=True):\n",
    "  \"\"\"activate/deactivate horizontal scrolling for wide output cells\"\"\"\n",
    "  from IPython.display import display, HTML\n",
    "  style = ('pre-wrap','pre')[activate] # select white-space style\n",
    "  display(HTML(\"<style>pre {white-space: %s !important}</style>\" % style))\n",
    "hscroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc5a8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+----------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|tip_amount|combined_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+----------+------------+\n",
      "|       1| 2011-01-01 15:44:50|  2011-01-01 16:13:58|              2|         17.2|         3|         170|           1|           1|      9.52|       73.02|\n",
      "|       2| 2011-01-05 05:42:00|  2011-01-05 06:07:00|              2|        17.54|         3|         186|           1|           1|      13.5|        75.5|\n",
      "|       2| 2011-01-05 15:46:00|  2011-01-05 16:21:00|              1|        17.52|         3|         170|           1|           2|       0.0|        60.3|\n",
      "|       1| 2011-01-07 15:00:47|  2011-01-07 15:26:37|              1|         14.5|         5|         186|           1|           1|     11.25|       86.25|\n",
      "|       2| 2011-01-11 11:12:00|  2011-01-11 11:41:00|              1|        16.05|         3|         234|           1|           2|       0.0|        63.1|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "file_path = f'gs://my-bigdata-project-cm/cleaned/yellow_tripdata_2023-12.parquet'\n",
    "try:\n",
    "        # Read Parquet file from Google Cloud Storage\n",
    "        sdf = spark.read.parquet(file_path)\n",
    "        sdf = sdf.drop('store_and_fwd_flag')\n",
    "        \n",
    "        # Sum up the numerical values of the trip fee columns\n",
    "        sdf = sdf.withColumn(\"combined_fee\", expr(\"total_amount + congestion_surcharge + airport_fee\"))\n",
    "            \n",
    "        # Drop the columns related to the trip fee, as they are now redundant\n",
    "        columns_to_drop = [\"fare_amount\",\"extra\",\"mta_tax\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\"congestion_surcharge\",\"airport_fee\"]\n",
    "        sdf = sdf.drop(*columns_to_drop)\n",
    "        \n",
    "        # Show the first row of the DataFrame\n",
    "        print(sdf.show(5))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred on {file_path}:\", str(e))\n",
    "    #spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5594bde",
   "metadata": {},
   "source": [
    "sdf.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac1078",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Print schema to see data types of all columns\n",
    "print(sdf.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2890f2",
   "metadata": {},
   "source": [
    "# Predict the passenger count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56943003",
   "metadata": {},
   "source": [
    "## Indexing(more like new column creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2266fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a hour column as a double\n",
    "def time_index_creator(sdf, input_column):\n",
    "    name = input_column.split(\"_\")[1]\n",
    "    \n",
    "    # Extract hour from datetime column\n",
    "    sdf = sdf.withColumn(f\"{name}_hour_index\", hour(input_column).cast(\"double\"))\n",
    "    \n",
    "    # Extract day of the week from datetime column\n",
    "    sdf = sdf.withColumn(f\"{name}_day_index\", day(input_column).cast(\"double\"))\n",
    "    \n",
    "    # Extract month from datetime column\n",
    "    sdf = sdf.withColumn(f\"{name}_month_index\", month(input_column).cast(\"double\"))\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "# Returns a DataFrame with a new column \"pickup_hour_index\" containing the indexed column of hours for the \"tpep_pickup_datetime\" column\n",
    "sdf = time_index_creator(sdf, \"tpep_pickup_datetime\")\n",
    "# Returns a DataFrame with a new column \"dropoff_hour_index\" containing the indexed column of hours for the \"tpep_dropoff_datetime\" column\n",
    "sdf = time_index_creator(sdf, \"tpep_dropoff_datetime\")\n",
    "\n",
    "# Calculate the duration of the trip in minutes and round it to the 2nd decimal place\n",
    "#sdf = sdf.withColumn(\"trip_length_minutes\", round((unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987aae2",
   "metadata": {},
   "source": [
    "## Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9872adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#removes rows were outliers are present in the selected column\n",
    "def remove_outliers(df, column_name, threshold=3):\n",
    "    # Calculate mean and standard deviation\n",
    "    mean_val = df.select(mean(col(column_name))).collect()[0][0]\n",
    "    std_val = df.select(stddev(col(column_name))).collect()[0][0]\n",
    "\n",
    "    # Calculate lower and upper bounds\n",
    "    lower_bound = mean_val - threshold * std_val\n",
    "    upper_bound = mean_val + threshold * std_val\n",
    "\n",
    "    # Filter out rows outside the threshold\n",
    "    filtered_df = df.filter((col(column_name) >= lower_bound) & (col(column_name) <= upper_bound))\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "#removes outliers from the passenger_count column\n",
    "sdf = remove_outliers(sdf, 'passenger_count')\n",
    "#removes outliers from the trip_distance column\n",
    "sdf = remove_outliers(sdf, 'trip_distance')\n",
    "#removes outliers from the RatecodeID column\n",
    "sdf = remove_outliers(sdf, 'RatecodeID')\n",
    "#removes outliers from the tip_amount column\n",
    "sdf = remove_outliers(sdf, 'tip_amount')\n",
    "#removes outliers from the combined_fee column\n",
    "sdf = remove_outliers(sdf, 'combined_fee')\n",
    "#removes outliers from the trip_length_minutes column\n",
    "#sdf = remove_outliers(sdf, 'trip_length_minutes')\n",
    "\n",
    "\n",
    "# Filter out rows where the absolute difference between dropoff_month_index and pickup_month_index is more than 2\n",
    "sdf = sdf.filter(abs(col(\"dropoff_month_index\") - col(\"pickup_month_index\")) <= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c600ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "trainingData, testData = sdf.randomSplit([0.70, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca319a22",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e362ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder for the three indexes and the age integer column.\n",
    "encoder = OneHotEncoder(inputCols=[\"pickup_hour_index\", \"dropoff_hour_index\",'pickup_day_index','dropoff_day_index','pickup_month_index','dropoff_month_index', \"VendorID\", \"RatecodeID\", \"PULocationID\", \"DOLocationID\", \"payment_type\", \"passenger_count\"],\n",
    "                        outputCols=[\"pickupHourVector\", \"dropoffHourVector\",'pickupDayVector','dropoffDayVector','pickupMonthVector','dropoffMonthVector', \"VendorIDVector\", \"RatecodeIDVector\", \"PULocationIDVector\", \"DOLocationIDVector\", \"paymentTypeVector\", \"passengerCountVector\"], dropLast=True, handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaca7d3",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7040528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale tripLengthVector\n",
    "#tripLength_scaler = MinMaxScaler(inputCol=\"tripLengthVector\", outputCol=\"tripLengthScaled\")\n",
    "\n",
    "#scale passengerCountVector\n",
    "passengerCount_scaler = MinMaxScaler(inputCol=\"passengerCountVector\", outputCol=\"passengerCountScaled\")\n",
    "\n",
    "#scale pickupHourVector\n",
    "pickupHour_scaler = MinMaxScaler(inputCol=\"pickupHourVector\", outputCol=\"pickupHourScaled\")\n",
    "\n",
    "#scale dropoffHourVector\n",
    "dropoffHour_scaler = MinMaxScaler(inputCol=\"dropoffHourVector\", outputCol=\"dropoffHourScaled\")\n",
    "\n",
    "#scale pickupDayVector\n",
    "pickupDay_scaler = MinMaxScaler(inputCol=\"pickupDayVector\", outputCol=\"pickupDayScaled\")\n",
    "\n",
    "#scale dropoffDayVector\n",
    "dropoffDay_scaler = MinMaxScaler(inputCol=\"dropoffDayVector\", outputCol=\"dropoffDayScaled\")\n",
    "\n",
    "#scale pickupMonthVector\n",
    "pickupMonth_scaler = MinMaxScaler(inputCol=\"pickupMonthVector\", outputCol=\"pickupMonthScaled\")\n",
    "\n",
    "#scale dropoffMonthVector\n",
    "dropoffMonth_scaler = MinMaxScaler(inputCol=\"dropoffMonthVector\", outputCol=\"dropoffMonthScaled\")\n",
    "\n",
    "# scale tip_amount(which is my label column)\n",
    "tip_amount_scaler = MinMaxScaler(inputCol=\"tip_amount\", outputCol=\"scaled_tip_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a2723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble and scale trip_distance\n",
    "distance_assembler = VectorAssembler(inputCols=['trip_distance'], outputCol='tripDistanceVector')\n",
    "distance_scaler = MinMaxScaler(inputCol=\"tripDistanceVector\", outputCol=\"tripDistanceScaled\")\n",
    "\n",
    "#assemble and scale combined_fee\n",
    "fee_assembler= VectorAssembler(inputCols=['combined_fee'], outputCol='combinedFeeVector')\n",
    "fee_scaler = MinMaxScaler(inputCol=\"combinedFeeVector\", outputCol=\"combinedFeeScaled\")\n",
    "\n",
    "#assemble and scale tip_amount(which is my label column)\n",
    "tip_amount_assembler = VectorAssembler(inputCols=['tip_amount'], outputCol='tipAmountVector')\n",
    "tip_amount_scaler = MinMaxScaler(inputCol=\"tipAmountVector\", outputCol=\"tipAmountScaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47b823",
   "metadata": {},
   "source": [
    "## Aassembling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522e15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an assembler for the individual feature vectors and the float/double columns\n",
    "assembler = VectorAssembler(inputCols=['VendorIDVector','RatecodeIDVector','PULocationIDVector','DOLocationIDVector','paymentTypeVector','passengerCountScaled','pickupHourScaled','dropoffHourScaled','pickupDayScaled', 'dropoffDayScaled', 'pickupMonthScaled', 'dropoffMonthScaled', 'tripDistanceScaled','combinedFeeScaled'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99477892",
   "metadata": {},
   "source": [
    "## Model building & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce5f6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column tipAmountScaled must be of type numeric but was actually of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m cv \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mregression_pipe, \n\u001b[1;32m     27\u001b[0m                     estimatorParamMaps\u001b[38;5;241m=\u001b[39mgrid, \n\u001b[1;32m     28\u001b[0m                     evaluator\u001b[38;5;241m=\u001b[39mevaluator, \n\u001b[1;32m     29\u001b[0m                     numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train the models\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m all_models  \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Show the average performance over the five folds for each grid combination\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage metric \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_models\u001b[38;5;241m.\u001b[39mavgMetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column tipAmountScaled must be of type numeric but was actually of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>>."
     ]
    }
   ],
   "source": [
    "# Create a Ridge Regression Estimator\n",
    "ridge_reg = LinearRegression(labelCol='tipAmountScaled',  elasticNetParam=0, regParam=0.1)\n",
    "\n",
    "# Create a regression evaluator (to get RMSE, R2, RME, etc.)\n",
    "evaluator = RegressionEvaluator(labelCol='tipAmountScaled')\n",
    "\n",
    "# Create the pipeline Indexer is stage 0 and Ridge Regression (ridge_reg)  is stage 3\n",
    "regression_pipe = Pipeline(stages=[encoder, passengerCount_scaler, pickupHour_scaler, dropoffHour_scaler, pickupDay_scaler, dropoffDay_scaler, pickupMonth_scaler, dropoffMonth_scaler, distance_assembler, distance_scaler, fee_assembler, fee_scaler, tip_amount_assembler, tip_amount_scaler, assembler, ridge_reg])\n",
    "\n",
    "# Create a grid to hold hyperparameters \n",
    "grid = ParamGridBuilder()\n",
    "\n",
    "# Two ways to try .fitIntercept\n",
    "params = ParamGridBuilder() \\\n",
    ".addGrid(ridge_reg.fitIntercept, [True, False]) \\\n",
    ".addGrid(ridge_reg.regParam, [0.001, 0.01, 0.1, 1, 10]) \\\n",
    ".addGrid(ridge_reg.elasticNetParam, [0, 0.25, 0.5, 0.75, 1]) \\\n",
    ".build()\n",
    "\n",
    "# Build the parameter grid\n",
    "grid = grid.build()\n",
    "\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "# Create the CrossValidator using the hyperparameter grid\n",
    "cv = CrossValidator(estimator=regression_pipe, \n",
    "                    estimatorParamMaps=grid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5,seed=42)\n",
    "\n",
    "# Train the models\n",
    "all_models  = cv.fit(trainingData)\n",
    "\n",
    "# Show the average performance over the five folds for each grid combination\n",
    "print(f\"Average metric {all_models.avgMetrics}\")\n",
    "\n",
    "# Get the best model from all of the models trained\n",
    "bestModel = all_models.bestModel\n",
    "\n",
    "# Use the model 'bestModel' to predict the test set\n",
    "test_results = bestModel.transform(testData)\n",
    "\n",
    "# Show the predicted tip\n",
    "test_results.select('tip_amount', 'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE measures the differences between what the model predicted ('prediction') and the actual values ('tip').\n",
    "rmse = evaluator.evaluate(test_results, {evaluator.metricName:'rmse'})\n",
    "# R-Squared measures how much of the variability in the target variable (tip) can be explained by the model\n",
    "r2 =evaluator.evaluate(test_results,{evaluator.metricName:'r2'})\n",
    "print(f\"RMSE: {rmse}  R-squared:{r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_path = \"gs://my-bigdata-project-cm/models/taxi_tip_linear_regression_model_v2\"\n",
    "bestModel.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce673e3",
   "metadata": {},
   "source": [
    "print(bestModel.stages)\n",
    "\n",
    "coefficients = bestModel.stages[3].coefficients\n",
    "print(\"bestModel coefficients\", coefficients)\n",
    "intercept = bestModel.stages[3].intercept\n",
    "print(\"bestModel intercept\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a105dc",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualize regression results\n",
    "\n",
    "# The Spark dataframe test_results holds the original 'tip' as well as the 'prediction'\n",
    "# Select and convert to a Pandas dataframe\n",
    "df = test_results.select('tip_amount','prediction').toPandas()\n",
    "\n",
    "# Set the style for Seaborn plots\n",
    "sns.set_style(\"white\")\n",
    " \n",
    "# Create a relationship plot between tip and prediction\n",
    "sns.lmplot(x='tip_amount', y='prediction', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc074c5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# residuals = bestModel.stages[3].residuals\n",
    "# test_results.residuals\n",
    "\n",
    "df = test_results.select('tip_amount','prediction').toPandas()\n",
    "df['residuals'] = df['tip_amount'] - df['prediction']\n",
    "\n",
    "# Set the style for Seaborn plots\n",
    "sns.set_style(\"white\")\n",
    " \n",
    "# Create a relationship plot between tip and prediction\n",
    "sns.regplot(x = 'prediction', y = 'residuals', data = df, scatter = True, color = 'red') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ce0dc",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO: Add more visualizations for Regression performance metrics\n",
    "# Loop through the features to extract the original column names. Store in the var_index dictionary\n",
    "var_index = dict()\n",
    "for variable_type in ['numeric', 'binary']:\n",
    "    for variable in test_results.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][variable_type]:\n",
    "         print(\"Found variable:\", variable)\n",
    "         idx = variable['idx']\n",
    "         name = variable['name']\n",
    "         var_index[idx] = name      # Add the name of the column to the dictionary\n",
    "\n",
    "# Loop through all of the variables found and print out the associated coefficients\n",
    "for i in range(len(var_index)):\n",
    "    print(i, var_index[i], coeff[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a178d2",
   "metadata": {},
   "source": [
    "# Close connection to Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
