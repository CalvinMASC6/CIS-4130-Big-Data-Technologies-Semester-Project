{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946aec1a",
   "metadata": {},
   "source": [
    "pip install pyspark\n",
    "pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d78e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spark functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col#, to_timestamp, lit, concat, udf, expr, round\n",
    "from pyspark.sql.column import Column#, _to_java_column, _to_seq\n",
    "from pyspark.sql.types import StringType, StructType, StructField, FloatType, IntegerType, BooleanType, TimestampType, DoubleType\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "# Import the storage module\n",
    "from google.cloud import storage\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3371f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Set the log level to ERROR to suppress INFO messages\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a28bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre {white-space: pre !important}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fix the formating of the shows, so they don't overlap.\n",
    "def hscroll(activate=True):\n",
    "  \"\"\"activate/deactivate horizontal scrolling for wide output cells\"\"\"\n",
    "  from IPython.display import display, HTML\n",
    "  style = ('pre-wrap','pre')[activate] # select white-space style\n",
    "  display(HTML(\"<style>pre {white-space: %s !important}</style>\" % style))\n",
    "hscroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84fba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_and_fwd_flag_filling(df): #cleans store_and_fwd_flag data\n",
    "\n",
    "    # Replace \"0\" and \"Null\" with \"False\" and \"1\" with \"True\" in store_and_fwd_flag column\n",
    "    df = df.withColumn(\"store_and_fwd_flag\",\n",
    "                       when((df[\"store_and_fwd_flag\"] == \"0\") | (df[\"store_and_fwd_flag\"] == \"N\"), False)\n",
    "                       .when((df[\"store_and_fwd_flag\"] == \"1\") | (df[\"store_and_fwd_flag\"] == \"Y\"), True) \n",
    "                       .otherwise(False)\n",
    "                       .cast(\"boolean\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da385e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_year(blob):\n",
    "    # Create a dictionary mapping month names to numbers\n",
    "    month_mapping = {\n",
    "        \"January\": \"01\",\n",
    "        \"February\": \"02\",\n",
    "        \"March\": \"03\",\n",
    "        \"April\": \"04\",\n",
    "        \"May\": \"05\",\n",
    "        \"June\": \"06\",\n",
    "        \"July\": \"07\",\n",
    "        \"August\": \"08\",\n",
    "        \"September\": \"09\",\n",
    "        \"October\": \"10\",\n",
    "        \"November\": \"11\",\n",
    "        \"December\": \"12\"\n",
    "    }\n",
    "\n",
    "    # Extract year and month name from the blob name\n",
    "    year_month = blob.name.split('/')[2].split('_')\n",
    "    year = int(year_month[0])\n",
    "    month_name = year_month[1].split('.')[0]\n",
    "\n",
    "    # Get the numeric value of the month from the dictionary\n",
    "    month_number = month_mapping.get(month_name)\n",
    "\n",
    "    return year, month_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a507a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_schema(df):\n",
    "    # Convert columns to the desired data types\n",
    "    df_transformed = df.withColumn(\"store_and_fwd_flag\", col(\"store_and_fwd_flag\").cast(BooleanType())) \\\n",
    "        .withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(TimestampType())) \\\n",
    "        .withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(TimestampType())) \\\n",
    "        .withColumn(\"VendorID\", col(\"VendorID\").cast(IntegerType())) \\\n",
    "        .withColumn(\"PULocationID\", col(\"PULocationID\").cast(IntegerType())) \\\n",
    "        .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(IntegerType())) \\\n",
    "        .withColumn(\"payment_type\", col(\"payment_type\").cast(IntegerType())) \\\n",
    "        .withColumn(\"passenger_count\", col(\"passenger_count\").cast(IntegerType())) \\\n",
    "        .withColumn(\"RatecodeID\", col(\"RatecodeID\").cast(IntegerType())) \\\n",
    "        .withColumn(\"fare_amount\", col(\"fare_amount\").cast(DoubleType())) \\\n",
    "        .withColumn(\"extra\", col(\"extra\").cast(DoubleType())) \\\n",
    "        .withColumn(\"mta_tax\", col(\"mta_tax\").cast(DoubleType())) \\\n",
    "        .withColumn(\"tip_amount\", col(\"tip_amount\").cast(DoubleType())) \\\n",
    "        .withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(DoubleType())) \\\n",
    "        .withColumn(\"improvement_surcharge\", col(\"improvement_surcharge\").cast(DoubleType())) \\\n",
    "        .withColumn(\"total_amount\", col(\"total_amount\").cast(DoubleType())) \\\n",
    "        .withColumn(\"congestion_surcharge\", col(\"congestion_surcharge\").cast(DoubleType())) \\\n",
    "        .withColumn(\"airport_fee\", col(\"airport_fee\").cast(DoubleType()))\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0b7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a client object that points to GCS\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Get a list of the 'blobs' (objects or files) in the bucket\n",
    "blobs = storage_client.list_blobs('my-bigdata-project-cm', prefix=\"landing/\")\n",
    "\n",
    "#run through the my-bigdata-project-cm bucket.\n",
    "for blob in blobs:\n",
    "    file_path = f'gs://my-bigdata-project-cm/{blob.name}'\n",
    "    \n",
    "    #exscluding the file title.\n",
    "    if not blob.name.endswith('.parquet') or blob.name == 'landing/':\n",
    "        print(f\"Skipping file {blob.name}\")\n",
    "        continue\n",
    "        \n",
    "    year, month = month_year(blob)#call returns the month and year of the file\n",
    "    \n",
    "    #if year <= 2010:# skip 2009 and 2010\n",
    "    #   print(f\"Skipping file {blob.name} from the year {year}\")\n",
    "    #   continue\n",
    "        \n",
    "    if year < 2017:#skipping those years i have already done in a previous batch\n",
    "        print(f\"Skipping file {blob.name} since it's already completed\")\n",
    "        continue\n",
    "    #if (year == 2013) and (month == \"04\" or month == \"08\"):\n",
    "    #   print(f\"Skipping file {blob.name} since it's already completed\")\n",
    "    #   continue\n",
    "    \n",
    "    try:\n",
    "        # Read Parquet file from Google Cloud Storage\n",
    "        df = spark.read.parquet(file_path)\n",
    "        \n",
    "        print(f\"Processing {blob.name}:\")\n",
    "        # Print the row count pre-cleaning.\n",
    "        #print(\"Number of rows pre-cleaning:\", df.count())\n",
    "\n",
    "        # Show the first row of the DataFrame pre-cleaning\n",
    "        #print(df.show(1))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred on {blob.name}:\", str(e))\n",
    "        continue\n",
    "\n",
    "    #drop duplicate column\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    #clean store_and_fwd_flag\n",
    "    df = store_and_fwd_flag_filling(df)\n",
    "\n",
    "    #change passenger_count to int\n",
    "    df = df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"int\"))\n",
    "\n",
    "    #change RatecodeID to int\n",
    "    df = df.withColumn(\"RatecodeID\", col(\"RatecodeID\").cast(\"int\"))\n",
    "\n",
    "    #Replace null values in the \"congestion_surcharge\" and \"Airport_fee\" columns with 0\n",
    "    df = df.fillna({'congestion_surcharge': 0.0, 'Airport_fee': 0.0})\n",
    "\n",
    "    # Drop rows where trip_distance is equal to 0.0\n",
    "    #df = df.filter(df[\"trip_distance\"] != 0.0) #leaving incase i want to look at cancled trips.\n",
    "\n",
    "    #drop all null rows.\n",
    "    df = df.dropna()\n",
    "\n",
    "    df = transform_schema(df)#enforce schema\n",
    "    \n",
    "    # Print the row count post-cleaning.\n",
    "    #print(\"Number of rows post-cleaning:\", df.count())\n",
    "\n",
    "    # Show the first row of the DataFrame post-cleaning\n",
    "    #print(df.show(1))\n",
    "    \n",
    "    # Print schema to see data types of all columns\n",
    "    #print(df.printSchema())\n",
    "    \n",
    "    # Save the cleaned dataframe as Parquet\n",
    "    output_file_path=f\"gs://my-bigdata-project-cm/cleaned/yellow_tripdata_{year}-{month}.parquet\"\n",
    "    df.write.parquet(output_file_path)\n",
    "    print(\"File successfully processed and uploaded.\\n\")\n",
    "print(\"All files finished processing.\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
